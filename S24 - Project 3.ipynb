{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4080f9",
   "metadata": {},
   "source": [
    "# CSC 84040 - Project 3 - Graph Mining\n",
    "\n",
    "## Project Description\n",
    "In this project you will perform network analysis on a subset of an Amazon co-purchasing network created by [Leskovec et al. (2007)](http://www.cs.cmu.edu/~jure/pubs/viral-tweb.pdf).  The dataset contains various product networks including books, DVDs, and videos. It was collected by crawling the Amazon website in March 2003 according to *Customers Who Bought This Item Also Bought* product relationships. Thus, if product *A* is always co-purchased with product *B*, the graph contains a **directed** edge from *A* to *B*.\n",
    "\n",
    "### Data\n",
    "There are **two data files** associated with this project:\n",
    "- *amazon_graph.txt*: This file contains the data for Part 1. It contains 16,552 observations (edges) of two (2) numbers representing product IDs. Each node represents a product and each directed edge between two nodes represents a co-purchase. The column fromNodeId contains the ID of the main purchasing item and ToNodeId contains the ID of the co-purchased items.\n",
    "- *ratings_train.csv*: This file contains the *training* data for Part 2. It contains 8,250 observations of Amazon product ratings. The `avg_rating` column contains the average rating by Amazon customers on a scale of 0-5.\n",
    "\n",
    "### Software Packages\n",
    "The following Python software packages are required to complete the project:\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "- `networkx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2a56d",
   "metadata": {},
   "source": [
    "## Part 1: Exploratory Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc78b24",
   "metadata": {},
   "source": [
    "Set the variable `GRAPH_FILE` to the **full path** to the Amazon product subgraph (**amazon_graph.txt**) on your system.\n",
    "\n",
    "Part 1 of the project is designed to help you familiarize yourself with the dataset and basic concepts of network analysis. The insights gleaned from Part 1 will help in building the predictive models in Part 2.\n",
    "\n",
    "### Network Analysis Tutorials\n",
    "1. Read the article [Social Network Analysis in Python](https://www.datacamp.com/community/tutorials/social-network-analysis-python) to understand the basics of (social) network snalysis.\n",
    "2. Review the [`NetworkX` Tutorial](https://networkx.org/documentation/stable/tutorial.html) to understand how you can use the `NetworkX` library to create and analyze graphs in Python.\n",
    "\n",
    "### Network Analysis\n",
    "After reading the above references, perform some basic network analyses and briefly explain each of your findings:\n",
    "1. Load the **directed** network graph ($G$) from the `GRAPH_FILE`. You may use `NetworkX`'s [`read_edgelist` function](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.edgelist.read_edgelist.html#read-edgelist) for this purpose.\n",
    "   How many items are present in the network and how many co-purchases are present?\n",
    "2. Compute the average shortest distance between all nodes in $G$. Explain your results briefly.\n",
    "3. Compute the *transitivity* and the average *clustering coefficient* of the network graph $G$. Explain your findings briefly based on the definitions of clustering coefficient and transitivity.\n",
    "4. Apply the PageRank algorithm to network $G$ with damping value 0.5 and find the 10 nodes with the highest PageRank. Explain your findings briefly.\n",
    "\n",
    "## Additional Resources\n",
    "1. [`NetworkX` API documentation](https://networkx.org/documentation/stable/reference/index.html)\n",
    "2. You are welcome to review my article on [applying network analysis techniques to the mathematics collaboration graph](https://www.ams.org/journals/notices/202205/rnoti-p849.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a11c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dced1",
   "metadata": {},
   "source": [
    "#### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c6f5ba-01b8-4a78-ae38-d88840d54216",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FILE = open(\"amazon_graph.txt\", 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb2fa2",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aeea633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 11001\n",
      "Number of co-purchases: 11025\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist(GRAPH_FILE)\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print(\"Number of items:\",num_nodes)\n",
    "print(\"Number of co-purchases:\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc330fc-f349-40b8-bc53-2b31d15d9ca7",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61532f2a-9596-46cd-afcd-000ffedf4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average shortest distance: 1.243934564029527\n"
     ]
    }
   ],
   "source": [
    "average_of = 0\n",
    "count = 0\n",
    "\n",
    "for C in (G.subgraph(c).copy() for c in nx.connected_components(G)):\n",
    "    average_of += nx.average_shortest_path_length(C)\n",
    "    count+=1\n",
    "\n",
    "print(\"Average shortest distance:\", average_of/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd806c9-810a-4d99-92a5-10b1048bd5ea",
   "metadata": {},
   "source": [
    "The distance here shows that an item is usually co-purshased and sometimes if they aren't co-purchased, then one item is related to another by sharing the same item they are co-purchased with since the distance is more than one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c75a6-0759-42fb-a6c4-f9653816b968",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "108e9392-1f5d-43f4-8c51-38b7824257bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitivity: 0.5251489080079418\n"
     ]
    }
   ],
   "source": [
    "print(\"Transitivity:\",nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9979bf-a503-4242-94e9-d8b12f7501db",
   "metadata": {},
   "source": [
    "The transtivity show that of the triads form (one item being co-purchased with one of two items), 0.525 of them have a chance that if an item is co-purchase with one of 2 items, then those two items are also likely to be co-purchased with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "934988ad-1e6f-49ab-a1ff-45aecfbf0698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average clustering coefficient: 0.4000347163833983\n"
     ]
    }
   ],
   "source": [
    "print(\"Average clustering coefficient:\",nx.average_clustering(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe01a6f2-5c1d-4a5a-8d0f-d6d87ce8042b",
   "metadata": {},
   "source": [
    "The average clustering coefficient indicate that given a subgraph with an item (co-purchases with that item), the average degree (coefficient to indicate completness of a graph) of all items in the subgraph being connected to each other is 0.4. 0.4 of the nodes in their subgraph won't make the subgraph complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0b9f8-2778-4992-af8c-87499c06cce1",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5395b13d-701d-4eb0-b105-e757fbc05f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4429', 0.00036442443488212214)\n",
      "('33', 0.0003598057652970889)\n",
      "('2501', 0.0003466387704726638)\n",
      "('7303', 0.00032008729015646807)\n",
      "('9106', 0.00028172941894020575)\n",
      "('3464', 0.00023908362410481076)\n",
      "('151', 0.0002334026017886055)\n",
      "('10745', 0.00021860125441267042)\n",
      "('16340', 0.0002110588786436082)\n",
      "('18', 0.00020506390788164124)\n"
     ]
    }
   ],
   "source": [
    "pr = nx.pagerank(G, alpha=0.5)\n",
    "pr_sorted = dict(sorted(pr.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#obtain top 10 items\n",
    "sorted_10 = list(pr_sorted.items())[:10]\n",
    "\n",
    "for element in sorted_10:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a69cd-4b09-4861-bb05-e1bc1c62e410",
   "metadata": {},
   "source": [
    "An item purchased will most likely direct a buyer to look at those 10 items. Item 4429 is most likely to get seen after looking at an item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1a3c8",
   "metadata": {},
   "source": [
    "## Part 2: Predict Product Ratings using Network-derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e9f37-6b72-4ec9-a040-fc9b11ed5c42",
   "metadata": {},
   "source": [
    "Set the variable `RATINGS_TRAIN_FILE` to the **full path** to the Amazon product ratings file *ratings_train.csv*.\n",
    "\n",
    "In this part of the project, you will build a machine learning model to predict the average rating of Amazon products on a scale of 0-5 using various network properties as features.\n",
    "\n",
    "The training dataset *ratings_train.csv* contains the product IDs and average ratings, the latter of which will be the targets ($y$ values) of your machine learning models.\n",
    "\n",
    "For each product ID in the training set, you need to extract at least **four (4)** different features based on its network properties to train your model. Some of the features that you can consider using include:\n",
    "- Clustering Coefficient\n",
    "- Page Rank\n",
    "- Degree centrality\n",
    "- Closeness centrality\n",
    "- Betweenness centrality\n",
    "- Degree of the node\n",
    "\n",
    "Using the extracted features, you will build at least **two (2)** regression models, where the input values ($x$ values) will be the extracted features for each product, and the regression targets ($y$ values) will be the average rating for said product. Some of the *regression* models that you can consider using include:\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Support Vector Machine (SVM)\n",
    "- Random forest\n",
    "- Multi-layer perceptron\n",
    "\n",
    "The main deliverable for Part 2 is a step-by-step analysis of your feature extraction, selection, and model building exercise, describing clearly how you generated features from your dataset and why you chose specific features over the others. You should evaluate the performance of multiple combinations of models and feature sets, using [mean absolute error (MAE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) as the performance metric.\n",
    "\n",
    "Your Jupyter notebook should contain the reproducible code for training various models as well as text descriptions of your conclusions after each step.\n",
    "\n",
    "Your grade on this part of the homework will depend on the accuracy of your model on a holdout test set (**not provided**) as well as your step-by-step description of how you arrived at your final model. We will evaluate your model using MAE on the test set.\n",
    "\n",
    "## Supervised Learnign in Scikit-learn\n",
    "Given a training dataset $(\\mathcal{X},\\mathcal{Y})$, the goal of **supervised learning** is to learn a function $f$ that maps inputs to outputs, i.e., $f:\\mathbf{x}\\rightarrow y$. For this problem, the $\\mathbf{x}\\in\\mathcal{X}$ values will be vectors of 4 or more features derived from the network, while the $y\\in \\mathcal{Y}$ values will be the average rating. The general process for training a model in `Scikit-learn` is as follows (using `LinearRegression` as an example):\n",
    "\n",
    "1. `lr = LinearRegression(fit_intercept=True) #Instantiate an object from the model class, setting any parameters as desired.`\n",
    "2. `lr.fit(X_train,Y_train) #Train the model using the training data.`\n",
    "3. `Y_train_pred = lr.predict(X_train) #Apply the model to a dataset, generating labels.`\n",
    "4. `mae_train = mean_absolute_error(Y_train,Y_train_pred) #Test model performance based on an appropriate metric.`\n",
    "\n",
    "Line 4 above measures the performance of the model on the same data that it was trained with. This is to gauge how well the model fit the data and provides a baseline for performance when you apply the model to the test set. Typically, model performance new, unseen data (e.g., the test set) will decrease. This decrease is the *generalization error* of the model.\n",
    "\n",
    "## Additional Resources\n",
    "- [Scikit-learn API](https://scikit-learn.org/stable/modules/classes.html)\n",
    "- [Scikit-learn User Guide: Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [Scikit-learn User Guide: Linear Models for Regression](https://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec785ef6-0d43-4366-ab91-aa845d3b7654",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da968ff-c075-4119-a11c-4a114ec47c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_TRAIN_FILE = pd.read_csv(\"ratings_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b0004-5464-4335-9411-e469a7f1fef7",
   "metadata": {},
   "source": [
    "#### Step 1: Create features to predict the average rating: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9014fe8-2523-4cb3-a86d-b6949b1fff5a",
   "metadata": {},
   "source": [
    "Feature to be created are:\n",
    "- The degree of centrality of each item\n",
    "- The degree of clossness for each item\n",
    "- The degree of betweeness for each item\n",
    "- The degree of an item\n",
    "- The rank of an item\n",
    "- The cluster coefficient of each item\n",
    "\n",
    "\n",
    "These features were created based on properties of nodes in a graph. The use of networkx returns the id of each item along with the property of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01644794-c0d3-4473-846d-7b01f520885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_cen = nx.degree_centrality(G)\n",
    "closeness_cen = nx.closeness_centrality(G)\n",
    "between_cen = nx.betweenness_centrality(G)\n",
    "degree = nx.degree(G)\n",
    "pr = nx.pagerank(G, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a546dd-f17d-4a69-a70c-e7205a05e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = nx.clustering(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745608f2-6296-4a4e-9395-5b063be112b1",
   "metadata": {},
   "source": [
    "#### Step 1.5: Create a dataframe with the train file and the created features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291831c9-06fc-4798-82ae-8abd05062971",
   "metadata": {},
   "source": [
    "- Create a dataframe of each of the created features.\n",
    "- This will then be merged with the training set where the items are the same for all of the created features.\n",
    "- Want to convert the id into a int to avoid any error with merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5441ade-aad6-4827-8fd3-3d58b86115f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert centrality measures and clustering coefficient to DataFrames\n",
    "degree_cen_df = pd.DataFrame(list(degree_cen.items()), columns=['id', 'degree_centrality'])\n",
    "closeness_cen_df = pd.DataFrame(list(closeness_cen.items()), columns=['id', 'closeness_centrality'])\n",
    "between_cen_df = pd.DataFrame(list(between_cen.items()), columns=['id', 'betweenness_centrality'])\n",
    "pr_df = pd.DataFrame(list(pr.items()), columns=['id', 'pagerank'])\n",
    "cluster_num_df = pd.DataFrame(list(cluster_num.items()), columns=['id', 'clustering_coefficient'])\n",
    "degree_df = pd.DataFrame(list(dict(degree).items()), columns=['id', 'degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0313d7c-add6-4f16-80b9-d279911fef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert centrality measures to DataFrames\n",
    "degree_cen_df['id'] = degree_cen_df['id'].astype('int64')\n",
    "closeness_cen_df['id'] = closeness_cen_df['id'].astype('int64')\n",
    "between_cen_df['id'] = between_cen_df['id'].astype('int64')\n",
    "pr_df['id'] = pr_df['id'].astype('int64')\n",
    "cluster_num_df['id'] = cluster_num_df['id'].astype('int64')\n",
    "\n",
    "# Convert degree to a DataFrame\n",
    "degree_df['id'] = degree_df['id'].astype('int64')\n",
    "\n",
    "# Merge all the DataFrames\n",
    "merged_df = RATINGS_TRAIN_FILE.merge(degree_cen_df, on='id').merge(closeness_cen_df, on='id').merge(between_cen_df, on='id').merge(pr_df, on='id').merge(cluster_num_df, on='id').merge(degree_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3d294b-edcd-4d1f-9b95-02c5294d4b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>clustering_coefficient</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93183</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231872</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67584</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>1.653043e-08</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116605</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33688</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8245</th>\n",
       "      <td>118615</td>\n",
       "      <td>Video</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8246</th>\n",
       "      <td>10235</td>\n",
       "      <td>Book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>6.612171e-08</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8247</th>\n",
       "      <td>102410</td>\n",
       "      <td>Book</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8248</th>\n",
       "      <td>161184</td>\n",
       "      <td>Book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8249</th>\n",
       "      <td>55584</td>\n",
       "      <td>Book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>8.265214e-09</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8250 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  group  avg_rating  degree_centrality  closeness_centrality  \\\n",
       "0      93183   Book         4.5           0.000091              0.000264   \n",
       "1     231872   Book         4.0           0.000182              0.000182   \n",
       "2      67584   Book         4.0           0.000182              0.000182   \n",
       "3     116605   Book         4.0           0.000182              0.000182   \n",
       "4      33688   Book         4.0           0.000182              0.000182   \n",
       "...      ...    ...         ...                ...                   ...   \n",
       "8245  118615  Video         3.5           0.000182              0.000182   \n",
       "8246   10235   Book         5.0           0.000273              0.000291   \n",
       "8247  102410   Book         4.0           0.000091              0.000091   \n",
       "8248  161184   Book         5.0           0.000182              0.000205   \n",
       "8249   55584   Book         5.0           0.000182              0.000207   \n",
       "\n",
       "      betweenness_centrality  pagerank  clustering_coefficient  degree  \n",
       "0               0.000000e+00  0.000064                0.000000       1  \n",
       "1               0.000000e+00  0.000091                1.000000       2  \n",
       "2               1.653043e-08  0.000119                0.000000       2  \n",
       "3               0.000000e+00  0.000091                1.000000       2  \n",
       "4               0.000000e+00  0.000091                1.000000       2  \n",
       "...                      ...       ...                     ...     ...  \n",
       "8245            0.000000e+00  0.000091                1.000000       2  \n",
       "8246            6.612171e-08  0.000113                0.333333       3  \n",
       "8247            0.000000e+00  0.000091                0.000000       1  \n",
       "8248            0.000000e+00  0.000088                1.000000       2  \n",
       "8249            8.265214e-09  0.000089                0.000000       2  \n",
       "\n",
       "[8250 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84068173-789d-4964-aefa-48b1d05dbb09",
   "metadata": {},
   "source": [
    "#### Step 2: Select features for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf42176-2dd6-447e-9091-891af0046d23",
   "metadata": {},
   "source": [
    "The features were selected based on the linear relationship is between with the avg_rating and with the other features.\n",
    "Features that didn't have a string relationship (close to 0 meant that the feature didn't have any relation with the data). The features chosen were degee centrality, betweeness centrality, page rank and clustering coefficient. Degree was excluded because of how similiar it is to degree centrality and closeness centrality was excluded becuase of how close it is to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea69ac0-bf07-432a-baa5-97945bd1b325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>clustering_coefficient</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>-0.012372</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.015388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_centrality</th>\n",
       "      <td>0.015388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277528</td>\n",
       "      <td>0.386463</td>\n",
       "      <td>0.670863</td>\n",
       "      <td>0.380754</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closeness_centrality</th>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.277528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325786</td>\n",
       "      <td>0.036011</td>\n",
       "      <td>-0.053539</td>\n",
       "      <td>0.277528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <td>-0.012372</td>\n",
       "      <td>0.386463</td>\n",
       "      <td>0.325786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305633</td>\n",
       "      <td>-0.062187</td>\n",
       "      <td>0.386463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pagerank</th>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.670863</td>\n",
       "      <td>0.036011</td>\n",
       "      <td>0.305633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.048652</td>\n",
       "      <td>0.670863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clustering_coefficient</th>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.380754</td>\n",
       "      <td>-0.053539</td>\n",
       "      <td>-0.062187</td>\n",
       "      <td>-0.048652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree</th>\n",
       "      <td>0.015388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277528</td>\n",
       "      <td>0.386463</td>\n",
       "      <td>0.670863</td>\n",
       "      <td>0.380754</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        avg_rating  degree_centrality  closeness_centrality  \\\n",
       "avg_rating                1.000000           0.015388             -0.000183   \n",
       "degree_centrality         0.015388           1.000000              0.277528   \n",
       "closeness_centrality     -0.000183           0.277528              1.000000   \n",
       "betweenness_centrality   -0.012372           0.386463              0.325786   \n",
       "pagerank                  0.007071           0.670863              0.036011   \n",
       "clustering_coefficient    0.008893           0.380754             -0.053539   \n",
       "degree                    0.015388           1.000000              0.277528   \n",
       "\n",
       "                        betweenness_centrality  pagerank  \\\n",
       "avg_rating                           -0.012372  0.007071   \n",
       "degree_centrality                     0.386463  0.670863   \n",
       "closeness_centrality                  0.325786  0.036011   \n",
       "betweenness_centrality                1.000000  0.305633   \n",
       "pagerank                              0.305633  1.000000   \n",
       "clustering_coefficient               -0.062187 -0.048652   \n",
       "degree                                0.386463  0.670863   \n",
       "\n",
       "                        clustering_coefficient    degree  \n",
       "avg_rating                            0.008893  0.015388  \n",
       "degree_centrality                     0.380754  1.000000  \n",
       "closeness_centrality                 -0.053539  0.277528  \n",
       "betweenness_centrality               -0.062187  0.386463  \n",
       "pagerank                             -0.048652  0.670863  \n",
       "clustering_coefficient                1.000000  0.380754  \n",
       "degree                                0.380754  1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "final_df = merged_df.drop(columns=['id','group']) #get rid of uneeded columns\n",
    "corr_matrix = final_df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b4809-b94d-4c39-ad51-d61c50dbd30e",
   "metadata": {},
   "source": [
    "#### Step 3: Experimenting data with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c72e9-3fdd-4b25-810b-cef33ab78516",
   "metadata": {},
   "source": [
    "Different regression models were used to test their performance with the dataset.\n",
    "\n",
    "The list of models experimented were:\n",
    "- Linear regression\n",
    "- Random forest regression\n",
    "- stocchastic gradient descent\n",
    "- Support vector machine (linear and rbf kernal)\n",
    "- multi-layer perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a893efed-3b9e-40e3-a6d5-11b4985fae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Partition data\n",
    "X = merged_df[['degree_centrality', 'betweenness_centrality', 'pagerank', 'clustering_coefficient']]\n",
    "y = merged_df['avg_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff86b7-c30e-48f2-8ad6-d0411b3ca639",
   "metadata": {},
   "source": [
    "For the use of a test set, test_size is the percentage of the test set with the merged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3823c51b-a1e8-4d58-a564-1de410d18936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf43227-8469-4247-919e-7e949f73d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d69c7016-6de2-4ce0-b4f4-3e1d3501a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def k_fold_cross_validation(model, X, y, n_folds=5, random_state=50):\n",
    "    \n",
    "    # Initialize k-fold cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize a list to store MAE for each fold\n",
    "    mae_scores = []\n",
    "    \n",
    "    # Loop over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        X_train_fold, X_val_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize the model\n",
    "        model_instance = model\n",
    "        \n",
    "        # Fit the model on the training data for this fold\n",
    "        model_instance.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict on the validation set for this fold\n",
    "        y_pred_fold = model_instance.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate the MAE for this fold and append it to the list of scores\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "        mae_scores.append(mae)\n",
    "    \n",
    "    # Calculate the average MAE across all folds\n",
    "    average_mae = np.mean(mae_scores)\n",
    "    \n",
    "    return average_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "507a1c0c-4eef-419d-88dc-f6dd984e18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5288786133532613\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "model_slm  = LinearRegression()\n",
    "average_mae_slm = k_fold_cross_validation(model_slm, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_slm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bcf5aa2-aa8e-4abe-bf2f-41d607dbaa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5431610507943818\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "model_rf = RandomForestRegressor()\n",
    "average_mae_rf = k_fold_cross_validation(model_rf, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "965f038a-c79f-4edc-ac0e-95692d7e64c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5065604516667258\n"
     ]
    }
   ],
   "source": [
    "#SVM Linear kernal\n",
    "model_svr = SVR(kernel = 'linear')\n",
    "average_mae_svr = k_fold_cross_validation(model_svr, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6588adcc-f8be-481b-896c-263078d9fa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.50654556756931\n"
     ]
    }
   ],
   "source": [
    "#SVM rbf kernal\n",
    "model_svr2 = SVR(kernel = 'rbf')\n",
    "average_mae_svr = k_fold_cross_validation(model_svr2, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "205069e1-5346-40b2-a7c9-d022b753574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5289805294331418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Stochastic gradient descent regressor\n",
    "sgd_regressor = SGDRegressor(learning_rate='constant', max_iter=8000, tol=1e-3, random_state=50)\n",
    "average_mae_sgd = k_fold_cross_validation(sgd_regressor, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0630b970-1e32-4273-80cb-b60b6a11551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5315212579149267\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "average_mae_gbr = k_fold_cross_validation(gbr, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d9c5e59-15a7-495a-b227-bc80bb3cd9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.5295793169366823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# MLP regressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100,50), activation='relu', solver='adam', max_iter=500, random_state=50)\n",
    "average_mae_mlp = k_fold_cross_validation(mlp_regressor, X_train_np, y_train_np)\n",
    "print(\"Average MAE:\", average_mae_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c524c2-6cb3-4441-b550-3a4d3c513cfa",
   "metadata": {},
   "source": [
    "#### Step 4: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e49225-66c2-4557-b6f1-8c68f7573702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR MAE: 0.5107267897779526\n"
     ]
    }
   ],
   "source": [
    "#SVM with rbf kernal\n",
    "model_svr2 = SVR(kernel = 'rbf')\n",
    "model_svr2.fit(X, y)\n",
    "y_pred = model_svr2.predict(X)\n",
    "print(\"SVR MAE:\", mean_absolute_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5a1ba-1ad0-48fe-af28-82653bbc1f60",
   "metadata": {},
   "source": [
    "For my model selection, I chose the support vector machine with a rbf kernal because of how low the average MAE was during cross validation; between the linear kernal and rbf kernal, the rbf was chosen since it can predict values based on non-linear boundaries. When applying the model, the MAE was also lower than most of the average MAE during cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeed728-1a2f-488b-b70a-43f2fdc7d49e",
   "metadata": {},
   "source": [
    "**In case a test set is used, changed the variables in case the data is partitioned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df648f98-2b25-4d0a-b107-7b87c737199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM with rbf kernal for partitioned data\n",
    "model_svr2 = SVR(kernel = 'rbf')\n",
    "model_svr2.fit(X_train, y_train)\n",
    "y_pred = model_svr2.predict(X_test)\n",
    "print(\"SVR MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
